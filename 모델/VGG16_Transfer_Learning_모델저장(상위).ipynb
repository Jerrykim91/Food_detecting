{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  전이학습 기반 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# a = np.array([[1,2,3],[1,2,3],[1,2,3]])\n",
    "# img = Image.fromarray(a)\n",
    "# b = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qpwpUhVod2ob"
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import rand\n",
    "pd.options.display.max_colwidth = 600\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cnn_utils as utils\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "          'axes.labelsize': 'x-large',\n",
    "          'axes.titlesize':'x-large',\n",
    "          'xtick.labelsize':'x-large',\n",
    "          'ytick.labelsize':'x-large'}\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1669,
     "status": "ok",
     "timestamp": 1531341784750,
     "user": {
      "displayName": "Raghav Bali",
      "photoUrl": "//lh4.googleusercontent.com/-HPass-4Bl9U/AAAAAAAAAAI/AAAAAAAAKiI/A0BQ8MHwVME/s50-c-k-no/photo.jpg",
      "userId": "117317575176939780509"
     },
     "user_tz": -330
    },
    "id": "VB1artr2KuLD",
    "outputId": "295301d0-a703-4793-fbc9-74c200f15189"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import callbacks, optimizers,Model\n",
    "from tensorflow.keras.applications import vgg16 as vgg\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 세트 로딩과 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fS8uGXn5dgRU"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "NUM_CLASSES = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "MOMENTUM = 0.9\n",
    "PATH=\"C:\\FoodClassification\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PoT9P1phLuyT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "base_model = vgg.VGG16(weights='imagenet', \n",
    "                       include_top=False, \n",
    "                       input_shape=(128, 128, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "khNrl8nHqesu"
   },
   "outputs": [],
   "source": [
    "# VGG16 모델의 세 번째 블록에서 마지막 층 추출\n",
    "# VGG16 모델의 네 번째 블록에서 마지막 층 추출\n",
    "last = base_model.get_layer('block4_pool').output\n",
    "# last = base_model.get_layer('block3_pool').output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 준비\n",
    "\n",
    "* 최상위층 없이 VGG16 로딩\n",
    "* 커스텀 분류기 준비\n",
    "* 모델의 맨 위에 새로운 층 쌓기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MI90lh6hL9ua"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 7,835,460\n",
      "Trainable params: 7,834,436\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 상위 층에 분류층 추가\n",
    "x = GlobalAveragePooling2D()(last)\n",
    "x= BatchNormalization()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.6)(x)\n",
    "pred = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "model = Model(base_model.input, pred)\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint('model_cnn3-{epoch:03d}-{acc:03f}-{val_acc:03f}.h5', verbose=1,\n",
    "                                             monitor='val_loss',\n",
    "                                             save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fc3EhfLTMD4I"
   },
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "     layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PRXPI3DCMIIK"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=LEARNING_RATE),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 814,
     "status": "ok",
     "timestamp": 1531341814064,
     "user": {
      "displayName": "Raghav Bali",
      "photoUrl": "//lh4.googleusercontent.com/-HPass-4Bl9U/AAAAAAAAAAI/AAAAAAAAKiI/A0BQ8MHwVME/s50-c-k-no/photo.jpg",
      "userId": "117317575176939780509"
     },
     "user_tz": -330
    },
    "id": "PATZIBLlMLrf",
    "outputId": "f9a4ac68-9261-46d8-a7c9-9e15cfcf5631"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "                 rescale=1/255,\n",
    "                validation_split=0.33\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_Y-jNseQMNcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13350 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        PATH,\n",
    "        shuffle=True,\n",
    "        seed=13,\n",
    "        target_size=(128,128),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode = 'categorical',\n",
    "        subset=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mHPmLOf-N3SQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6574 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = train_datagen.flow_from_directory(\n",
    "        PATH,\n",
    "        shuffle=True,\n",
    "        seed=13,\n",
    "        target_size=(128,128),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode = 'categorical',\n",
    "        subset=\"validation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JSIJycdbrBWK",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.5280 - acc: 0.7637Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.4603 - acc: 0.7923\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.46026, saving model to model_cnn3-001-0.764141-0.792305.h5\n",
      "100/100 [==============================] - 457s 5s/step - loss: 0.5271 - acc: 0.7641 - val_loss: 0.4603 - val_acc: 0.7923\n",
      "Epoch 2/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.4203 - acc: 0.8130Epoch 1/100\n",
      "100/100 [==============================] - 234s 2s/step - loss: 0.4090 - acc: 0.8170\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.46026 to 0.40898, saving model to model_cnn3-002-0.813281-0.817031.h5\n",
      "100/100 [==============================] - 480s 5s/step - loss: 0.4197 - acc: 0.8133 - val_loss: 0.4090 - val_acc: 0.8170\n",
      "Epoch 3/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.3642 - acc: 0.8408Epoch 1/100\n",
      "100/100 [==============================] - 232s 2s/step - loss: 0.3760 - acc: 0.8319\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.40898 to 0.37603, saving model to model_cnn3-003-0.841092-0.831875.h5\n",
      "100/100 [==============================] - 469s 5s/step - loss: 0.3636 - acc: 0.8411 - val_loss: 0.3760 - val_acc: 0.8319\n",
      "Epoch 4/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.3378 - acc: 0.8557Epoch 1/100\n",
      "100/100 [==============================] - 229s 2s/step - loss: 0.3582 - acc: 0.8405\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.37603 to 0.35820, saving model to model_cnn3-004-0.856211-0.840469.h5\n",
      "100/100 [==============================] - 457s 5s/step - loss: 0.3369 - acc: 0.8562 - val_loss: 0.3582 - val_acc: 0.8405\n",
      "Epoch 5/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.3126 - acc: 0.8670Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3506 - acc: 0.8421\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35820 to 0.35061, saving model to model_cnn3-005-0.866758-0.842109.h5\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.3130 - acc: 0.8668 - val_loss: 0.3506 - val_acc: 0.8421\n",
      "Epoch 6/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.2915 - acc: 0.8741Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3391 - acc: 0.8469\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35061 to 0.33910, saving model to model_cnn3-006-0.874019-0.846875.h5\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.2918 - acc: 0.8740 - val_loss: 0.3391 - val_acc: 0.8469\n",
      "Epoch 7/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.2714 - acc: 0.8842Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3398 - acc: 0.8465\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.33910\n",
      "100/100 [==============================] - 452s 5s/step - loss: 0.2715 - acc: 0.8841 - val_loss: 0.3398 - val_acc: 0.8465\n",
      "Epoch 8/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.2676 - acc: 0.8899Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3348 - acc: 0.8491\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.33910 to 0.33478, saving model to model_cnn3-008-0.889648-0.849102.h5\n",
      "100/100 [==============================] - 453s 5s/step - loss: 0.2681 - acc: 0.8896 - val_loss: 0.3348 - val_acc: 0.8491\n",
      "Epoch 9/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.2522 - acc: 0.8941Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3287 - acc: 0.8517\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.33478 to 0.32869, saving model to model_cnn3-009-0.894258-0.851680.h5\n",
      "100/100 [==============================] - 452s 5s/step - loss: 0.2522 - acc: 0.8943 - val_loss: 0.3287 - val_acc: 0.8517\n",
      "Epoch 10/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.2475 - acc: 0.8984Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3277 - acc: 0.8526\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.32869 to 0.32769, saving model to model_cnn3-010-0.898750-0.852617.h5\n",
      "100/100 [==============================] - 453s 5s/step - loss: 0.2467 - acc: 0.8988 - val_loss: 0.3277 - val_acc: 0.8526\n",
      "Epoch 11/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.2303 - acc: 0.9048Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3258 - acc: 0.8541\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.32769 to 0.32582, saving model to model_cnn3-011-0.904926-0.854062.h5\n",
      "100/100 [==============================] - 452s 5s/step - loss: 0.2302 - acc: 0.9049 - val_loss: 0.3258 - val_acc: 0.8541\n",
      "Epoch 12/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.2336 - acc: 0.9045Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3302 - acc: 0.8523\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.32582\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.2331 - acc: 0.9047 - val_loss: 0.3302 - val_acc: 0.8523\n",
      "Epoch 13/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.2227 - acc: 0.9092Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3246 - acc: 0.8535\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.32582 to 0.32460, saving model to model_cnn3-013-0.908945-0.853477.h5\n",
      "100/100 [==============================] - 453s 5s/step - loss: 0.2229 - acc: 0.9089 - val_loss: 0.3246 - val_acc: 0.8535\n",
      "Epoch 14/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.2082 - acc: 0.9156Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3230 - acc: 0.8562\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.32460 to 0.32302, saving model to model_cnn3-014-0.915937-0.856250.h5\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.2074 - acc: 0.9159 - val_loss: 0.3230 - val_acc: 0.8562\n",
      "Epoch 15/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.2050 - acc: 0.9177Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3230 - acc: 0.8564\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.32302 to 0.32296, saving model to model_cnn3-015-0.917948-0.856445.h5\n",
      "100/100 [==============================] - 453s 5s/step - loss: 0.2045 - acc: 0.9179 - val_loss: 0.3230 - val_acc: 0.8564\n",
      "Epoch 16/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1984 - acc: 0.9192Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3164 - acc: 0.8598\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.32296 to 0.31635, saving model to model_cnn3-016-0.919102-0.859766.h5\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.1988 - acc: 0.9191 - val_loss: 0.3164 - val_acc: 0.8598\n",
      "Epoch 17/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1940 - acc: 0.9237Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3228 - acc: 0.8571\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.31635\n",
      "100/100 [==============================] - 453s 5s/step - loss: 0.1938 - acc: 0.9235 - val_loss: 0.3228 - val_acc: 0.8571\n",
      "Epoch 18/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1864 - acc: 0.9248Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3181 - acc: 0.8595\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.31635\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.1873 - acc: 0.9243 - val_loss: 0.3181 - val_acc: 0.8595\n",
      "Epoch 19/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1871 - acc: 0.9246Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3210 - acc: 0.8595\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.31635\n",
      "100/100 [==============================] - 453s 5s/step - loss: 0.1870 - acc: 0.9247 - val_loss: 0.3210 - val_acc: 0.8595\n",
      "Epoch 20/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1742 - acc: 0.9315Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3205 - acc: 0.8607\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.31635\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.1745 - acc: 0.9313 - val_loss: 0.3205 - val_acc: 0.8607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1793 - acc: 0.9292Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3153 - acc: 0.8618\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.31635 to 0.31533, saving model to model_cnn3-021-0.929165-0.861836.h5\n",
      "100/100 [==============================] - 453s 5s/step - loss: 0.1789 - acc: 0.9292 - val_loss: 0.3153 - val_acc: 0.8618\n",
      "Epoch 22/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1651 - acc: 0.9349Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3234 - acc: 0.8606\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.1652 - acc: 0.9350 - val_loss: 0.3234 - val_acc: 0.8606\n",
      "Epoch 23/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1632 - acc: 0.9339Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3241 - acc: 0.8618\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.1630 - acc: 0.9341 - val_loss: 0.3241 - val_acc: 0.8618\n",
      "Epoch 24/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1532 - acc: 0.9406Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3244 - acc: 0.8611\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.1527 - acc: 0.9409 - val_loss: 0.3244 - val_acc: 0.8611\n",
      "Epoch 25/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1561 - acc: 0.9391Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3262 - acc: 0.8616\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.1561 - acc: 0.9388 - val_loss: 0.3262 - val_acc: 0.8616\n",
      "Epoch 26/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1455 - acc: 0.9449Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3191 - acc: 0.8654\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.1459 - acc: 0.9448 - val_loss: 0.3191 - val_acc: 0.8654\n",
      "Epoch 27/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1484 - acc: 0.9429Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3270 - acc: 0.8630\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.1486 - acc: 0.9427 - val_loss: 0.3270 - val_acc: 0.8630\n",
      "Epoch 28/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1411 - acc: 0.9437Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3291 - acc: 0.8624\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.1411 - acc: 0.9436 - val_loss: 0.3291 - val_acc: 0.8624\n",
      "Epoch 29/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1440 - acc: 0.9441Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3245 - acc: 0.8637\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.1434 - acc: 0.9444 - val_loss: 0.3245 - val_acc: 0.8637\n",
      "Epoch 30/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1366 - acc: 0.9461Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3293 - acc: 0.8628\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.1363 - acc: 0.9461 - val_loss: 0.3293 - val_acc: 0.8628\n",
      "Epoch 31/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1336 - acc: 0.9498Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3327 - acc: 0.8622\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.1346 - acc: 0.9496 - val_loss: 0.3327 - val_acc: 0.8622\n",
      "Epoch 32/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1276 - acc: 0.9502Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3283 - acc: 0.8651\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.1275 - acc: 0.9504 - val_loss: 0.3283 - val_acc: 0.8651\n",
      "Epoch 33/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1324 - acc: 0.9502Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3268 - acc: 0.8648\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.1327 - acc: 0.9500 - val_loss: 0.3268 - val_acc: 0.8648\n",
      "Epoch 34/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1202 - acc: 0.9536Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3243 - acc: 0.8662\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.1205 - acc: 0.9533 - val_loss: 0.3243 - val_acc: 0.8662\n",
      "Epoch 35/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1216 - acc: 0.9550Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3276 - acc: 0.8663\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.1219 - acc: 0.9551 - val_loss: 0.3276 - val_acc: 0.8663\n",
      "Epoch 36/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1157 - acc: 0.9568Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3223 - acc: 0.8693\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.1158 - acc: 0.9567 - val_loss: 0.3223 - val_acc: 0.8693\n",
      "Epoch 37/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1175 - acc: 0.9571Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3343 - acc: 0.8655\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.1171 - acc: 0.9573 - val_loss: 0.3343 - val_acc: 0.8655\n",
      "Epoch 38/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1148 - acc: 0.9559Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3420 - acc: 0.8627\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.1148 - acc: 0.9560 - val_loss: 0.3420 - val_acc: 0.8627\n",
      "Epoch 39/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1144 - acc: 0.9559Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3333 - acc: 0.8662\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.1144 - acc: 0.9559 - val_loss: 0.3333 - val_acc: 0.8662\n",
      "Epoch 40/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1061 - acc: 0.9607Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3368 - acc: 0.8679\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.1061 - acc: 0.9605 - val_loss: 0.3368 - val_acc: 0.8679\n",
      "Epoch 41/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1059 - acc: 0.9608Epoch 1/100\n",
      "100/100 [==============================] - 227s 2s/step - loss: 0.3410 - acc: 0.8661\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.1062 - acc: 0.9606 - val_loss: 0.3410 - val_acc: 0.8661\n",
      "Epoch 42/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.1020 - acc: 0.9622Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3453 - acc: 0.8646\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.1020 - acc: 0.9621 - val_loss: 0.3453 - val_acc: 0.8646\n",
      "Epoch 43/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0988 - acc: 0.9635Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3453 - acc: 0.8654\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.0988 - acc: 0.9634 - val_loss: 0.3453 - val_acc: 0.8654\n",
      "Epoch 44/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0983 - acc: 0.9631Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3465 - acc: 0.8665\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0982 - acc: 0.9630 - val_loss: 0.3465 - val_acc: 0.8665\n",
      "Epoch 45/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0935 - acc: 0.9651Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3487 - acc: 0.8651\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0938 - acc: 0.9649 - val_loss: 0.3487 - val_acc: 0.8651\n",
      "Epoch 46/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0956 - acc: 0.9628Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3474 - acc: 0.8666\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0957 - acc: 0.9627 - val_loss: 0.3474 - val_acc: 0.8666\n",
      "Epoch 47/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0849 - acc: 0.9682Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3494 - acc: 0.8667\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0854 - acc: 0.9680 - val_loss: 0.3494 - val_acc: 0.8667\n",
      "Epoch 48/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0910 - acc: 0.9659Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3485 - acc: 0.8675\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0917 - acc: 0.9656 - val_loss: 0.3485 - val_acc: 0.8675\n",
      "Epoch 49/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0880 - acc: 0.9689Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3461 - acc: 0.8673\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0880 - acc: 0.9689 - val_loss: 0.3461 - val_acc: 0.8673\n",
      "Epoch 50/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0887 - acc: 0.9669Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3562 - acc: 0.8655\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.0890 - acc: 0.9667 - val_loss: 0.3562 - val_acc: 0.8655\n",
      "Epoch 51/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0867 - acc: 0.9687Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3540 - acc: 0.8691\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0865 - acc: 0.9689 - val_loss: 0.3540 - val_acc: 0.8691\n",
      "Epoch 52/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0865 - acc: 0.9677Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3552 - acc: 0.8685\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0866 - acc: 0.9677 - val_loss: 0.3552 - val_acc: 0.8685\n",
      "Epoch 53/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0797 - acc: 0.9718Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3592 - acc: 0.8685\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0795 - acc: 0.9720 - val_loss: 0.3592 - val_acc: 0.8685\n",
      "Epoch 54/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0774 - acc: 0.9738Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3534 - acc: 0.8698\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0773 - acc: 0.9738 - val_loss: 0.3534 - val_acc: 0.8698\n",
      "Epoch 55/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0806 - acc: 0.9718Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3657 - acc: 0.8653\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0807 - acc: 0.9717 - val_loss: 0.3657 - val_acc: 0.8653\n",
      "Epoch 56/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0748 - acc: 0.9733Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3617 - acc: 0.8682\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0745 - acc: 0.9734 - val_loss: 0.3617 - val_acc: 0.8682\n",
      "Epoch 57/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0715 - acc: 0.9757Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3764 - acc: 0.8647\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0712 - acc: 0.9758 - val_loss: 0.3764 - val_acc: 0.8647\n",
      "Epoch 58/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0729 - acc: 0.9733Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3720 - acc: 0.8674\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0729 - acc: 0.9733 - val_loss: 0.3720 - val_acc: 0.8674\n",
      "Epoch 59/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0627 - acc: 0.9772Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3894 - acc: 0.8639\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.0631 - acc: 0.9769 - val_loss: 0.3894 - val_acc: 0.8639\n",
      "Epoch 60/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0689 - acc: 0.9752Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3722 - acc: 0.8689\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0687 - acc: 0.9753 - val_loss: 0.3722 - val_acc: 0.8689\n",
      "Epoch 61/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0672 - acc: 0.9759Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3830 - acc: 0.8649\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0672 - acc: 0.9760 - val_loss: 0.3830 - val_acc: 0.8649\n",
      "Epoch 62/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0624 - acc: 0.9786Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3889 - acc: 0.8622\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0631 - acc: 0.9783 - val_loss: 0.3889 - val_acc: 0.8622\n",
      "Epoch 63/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0645 - acc: 0.9767Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3876 - acc: 0.8646\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0648 - acc: 0.9765 - val_loss: 0.3876 - val_acc: 0.8646\n",
      "Epoch 64/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0638 - acc: 0.9767Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3859 - acc: 0.8681\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0639 - acc: 0.9767 - val_loss: 0.3859 - val_acc: 0.8681\n",
      "Epoch 65/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0581 - acc: 0.9794Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3743 - acc: 0.8719\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0582 - acc: 0.9794 - val_loss: 0.3743 - val_acc: 0.8719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0567 - acc: 0.9791Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3859 - acc: 0.8706\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0569 - acc: 0.9790 - val_loss: 0.3859 - val_acc: 0.8706\n",
      "Epoch 67/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0593 - acc: 0.9796Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3925 - acc: 0.8680\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0590 - acc: 0.9798 - val_loss: 0.3925 - val_acc: 0.8680\n",
      "Epoch 68/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0555 - acc: 0.9799Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3997 - acc: 0.8663\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0553 - acc: 0.9801 - val_loss: 0.3997 - val_acc: 0.8663\n",
      "Epoch 69/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0567 - acc: 0.9797Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3903 - acc: 0.8702\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0567 - acc: 0.9797 - val_loss: 0.3903 - val_acc: 0.8702\n",
      "Epoch 70/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0499 - acc: 0.9832Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3918 - acc: 0.8688\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0497 - acc: 0.9833 - val_loss: 0.3918 - val_acc: 0.8688\n",
      "Epoch 71/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0542 - acc: 0.9800Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3982 - acc: 0.8680\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0546 - acc: 0.9798 - val_loss: 0.3982 - val_acc: 0.8680\n",
      "Epoch 72/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0484 - acc: 0.9835Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.3952 - acc: 0.8698\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0483 - acc: 0.9836 - val_loss: 0.3952 - val_acc: 0.8698\n",
      "Epoch 73/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0513 - acc: 0.9821Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4057 - acc: 0.8694\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.0511 - acc: 0.9823 - val_loss: 0.4057 - val_acc: 0.8694\n",
      "Epoch 74/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0444 - acc: 0.9843Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4074 - acc: 0.8697\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0442 - acc: 0.9845 - val_loss: 0.4074 - val_acc: 0.8697\n",
      "Epoch 75/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0510 - acc: 0.9815Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4045 - acc: 0.8712\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0510 - acc: 0.9815 - val_loss: 0.4045 - val_acc: 0.8712\n",
      "Epoch 76/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0442 - acc: 0.9848Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4305 - acc: 0.8634\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.0442 - acc: 0.9848 - val_loss: 0.4305 - val_acc: 0.8634\n",
      "Epoch 77/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0486 - acc: 0.9824Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4129 - acc: 0.8686\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0488 - acc: 0.9824 - val_loss: 0.4129 - val_acc: 0.8686\n",
      "Epoch 78/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0452 - acc: 0.9845Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4252 - acc: 0.8653\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0455 - acc: 0.9843 - val_loss: 0.4252 - val_acc: 0.8653\n",
      "Epoch 79/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0455 - acc: 0.9845Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4268 - acc: 0.8655\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 454s 5s/step - loss: 0.0457 - acc: 0.9845 - val_loss: 0.4268 - val_acc: 0.8655\n",
      "Epoch 80/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0403 - acc: 0.9858Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4261 - acc: 0.8682\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0403 - acc: 0.9858 - val_loss: 0.4261 - val_acc: 0.8682\n",
      "Epoch 81/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0400 - acc: 0.9867Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4322 - acc: 0.8669\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0399 - acc: 0.9869 - val_loss: 0.4322 - val_acc: 0.8669\n",
      "Epoch 82/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0415 - acc: 0.9854Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4310 - acc: 0.8682\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0415 - acc: 0.9853 - val_loss: 0.4310 - val_acc: 0.8682\n",
      "Epoch 83/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0413 - acc: 0.9861Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4247 - acc: 0.8704\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0412 - acc: 0.9861 - val_loss: 0.4247 - val_acc: 0.8704\n",
      "Epoch 84/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0356 - acc: 0.9884Epoch 1/100\n",
      "100/100 [==============================] - 230s 2s/step - loss: 0.4460 - acc: 0.8634\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 458s 5s/step - loss: 0.0355 - acc: 0.9885 - val_loss: 0.4460 - val_acc: 0.8634\n",
      "Epoch 85/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0371 - acc: 0.9877Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4465 - acc: 0.8663\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0370 - acc: 0.9877 - val_loss: 0.4465 - val_acc: 0.8663\n",
      "Epoch 86/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0349 - acc: 0.9878Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4489 - acc: 0.8676\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0349 - acc: 0.9878 - val_loss: 0.4489 - val_acc: 0.8676\n",
      "Epoch 87/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0361 - acc: 0.9882Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4467 - acc: 0.8672\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0361 - acc: 0.9882 - val_loss: 0.4467 - val_acc: 0.8672\n",
      "Epoch 88/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0345 - acc: 0.9888Epoch 1/100\n",
      "100/100 [==============================] - 229s 2s/step - loss: 0.4505 - acc: 0.8677\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 457s 5s/step - loss: 0.0345 - acc: 0.9888 - val_loss: 0.4505 - val_acc: 0.8677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0331 - acc: 0.9891Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4469 - acc: 0.8688\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0329 - acc: 0.9892 - val_loss: 0.4469 - val_acc: 0.8688\n",
      "Epoch 90/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0347 - acc: 0.9890Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4470 - acc: 0.8684\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0346 - acc: 0.9890 - val_loss: 0.4470 - val_acc: 0.8684\n",
      "Epoch 91/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0288 - acc: 0.9903Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4511 - acc: 0.8671\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0287 - acc: 0.9904 - val_loss: 0.4511 - val_acc: 0.8671\n",
      "Epoch 92/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0314 - acc: 0.9895Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4702 - acc: 0.8669\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0317 - acc: 0.9894 - val_loss: 0.4702 - val_acc: 0.8669\n",
      "Epoch 93/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0299 - acc: 0.9903Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4665 - acc: 0.8662\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0300 - acc: 0.9904 - val_loss: 0.4665 - val_acc: 0.8662\n",
      "Epoch 94/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0310 - acc: 0.9899Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4540 - acc: 0.8692\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0309 - acc: 0.9900 - val_loss: 0.4540 - val_acc: 0.8692\n",
      "Epoch 95/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0304 - acc: 0.9907Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4654 - acc: 0.8684\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0302 - acc: 0.9908 - val_loss: 0.4654 - val_acc: 0.8684\n",
      "Epoch 96/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0309 - acc: 0.9900Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4715 - acc: 0.8675\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0314 - acc: 0.9897 - val_loss: 0.4715 - val_acc: 0.8675\n",
      "Epoch 97/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0251 - acc: 0.9916Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4723 - acc: 0.8680\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0251 - acc: 0.9916 - val_loss: 0.4723 - val_acc: 0.8680\n",
      "Epoch 98/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0276 - acc: 0.9912Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4694 - acc: 0.8698\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 455s 5s/step - loss: 0.0276 - acc: 0.9911 - val_loss: 0.4694 - val_acc: 0.8698\n",
      "Epoch 99/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0278 - acc: 0.9912Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4863 - acc: 0.8679\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0279 - acc: 0.9910 - val_loss: 0.4863 - val_acc: 0.8679\n",
      "Epoch 100/100\n",
      " 99/100 [============================>.] - ETA: 2s - loss: 0.0243 - acc: 0.9925Epoch 1/100\n",
      "100/100 [==============================] - 228s 2s/step - loss: 0.4824 - acc: 0.8684\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.31533\n",
      "100/100 [==============================] - 456s 5s/step - loss: 0.0241 - acc: 0.9925 - val_loss: 0.4824 - val_acc: 0.8684\n"
     ]
    }
   ],
   "source": [
    "train_steps_per_epoch =100\n",
    "val_steps_per_epoch = 100\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=train_steps_per_epoch,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=val_steps_per_epoch,\n",
    "                              epochs=EPOCHS,\n",
    "                              verbose=1, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('VGG16.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "손실값이 일덩수준이 지나면 증가함 그래서 최소값을가지는 그리고 최대 정학도를 가지는 모데를 우리는 취해야함 \n",
    "\n",
    "학습시키면서 자동으로 콜백함수가ㅜ걸렷음  그래소 11에폭이후로는 붚필요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./vgg16_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('vgg16.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\admin\\Anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pydot\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    graphviz-2.38              |       hfd603c8_2        37.7 MB\n",
      "    pydot-1.4.1                |           py36_0          43 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        37.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  graphviz           pkgs/main/win-64::graphviz-2.38-hfd603c8_2\n",
      "  pydot              pkgs/main/win-64::pydot-1.4.1-py36_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "graphviz-2.38        | 37.7 MB   |            |   0% \n",
      "graphviz-2.38        | 37.7 MB   |            |   0% \n",
      "graphviz-2.38        | 37.7 MB   | 2          |   2% \n",
      "graphviz-2.38        | 37.7 MB   | #          |  10% \n",
      "graphviz-2.38        | 37.7 MB   | #4         |  14% \n",
      "graphviz-2.38        | 37.7 MB   | ##5        |  25% \n",
      "graphviz-2.38        | 37.7 MB   | ###3       |  33% \n",
      "graphviz-2.38        | 37.7 MB   | ####1      |  42% \n",
      "graphviz-2.38        | 37.7 MB   | ####8      |  48% \n",
      "graphviz-2.38        | 37.7 MB   | #####6     |  56% \n",
      "graphviz-2.38        | 37.7 MB   | ######4    |  64% \n",
      "graphviz-2.38        | 37.7 MB   | #######2   |  72% \n",
      "graphviz-2.38        | 37.7 MB   | ########   |  81% \n",
      "graphviz-2.38        | 37.7 MB   | ########8  |  89% \n",
      "graphviz-2.38        | 37.7 MB   | #########7 |  97% \n",
      "graphviz-2.38        | 37.7 MB   | ########## | 100% \n",
      "\n",
      "pydot-1.4.1          | 43 KB     |            |   0% \n",
      "pydot-1.4.1          | 43 KB     | ###6       |  37% \n",
      "pydot-1.4.1          | 43 KB     | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Copy of cifar10_vgg16_july12.ipynb",
   "provenance": [
    {
     "file_id": "1tlQq2thZWPDNqaMbfBJ9_TOo0YTqv7IL",
     "timestamp": 1531320536438
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
